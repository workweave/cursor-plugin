---
description: Guidelines for using Weave engineering analytics MCP tools effectively
alwaysApply: true
---

# Weave Engineering Analytics

You have access to the Weave MCP server, which provides real-time engineering metrics and analytics. Use these tools to answer questions about team productivity, code velocity, review quality, and AI code adoption.

## Available tools

### get_metric_overview
Aggregated metrics with trends, time series, and benchmarks. Use for high-level questions like "How is our team doing?" or "What's our PR cycle time trend?"

### get_metric_drill_down
Detailed records behind a metric. Use when investigating specific data points, finding outliers, or examining individual PRs/tasks/reviews.

### get_accounts
List team members with their names and IDs. Call this first when you need to filter metrics by specific people.

### get_teams
List teams with their names and IDs. Call this first when you need to filter metrics by team.

## Metric types

Key metrics available (use exact values for `metric_type`):

**Code velocity:**
- `code_output` — Weighted code output score
- `code_loc` — Lines of code changed
- `prs` — Pull requests merged
- `pr_cycle_time` — Time from PR open to merge
- `pr_merge_time` — Time from approval to merge

**Code review:**
- `code_reviews` — Number of code reviews
- `code_review_turnaround` — Time to first review
- `code_review_quality` — Review quality score
- `review_cycles` — Rounds of review before merge
- `pr_review_rate` — Percentage of PRs reviewed

**Task delivery:**
- `tasks` — Tasks completed
- `points` — Story points delivered
- `task_lead_time` — Time from task creation to completion
- `task_delivery` — Task delivery rate

**AI code:**
- `ai_code_loc` — AI-assisted lines of code
- `ai_code_percentage` — Percentage of code written with AI
- `ai_output_percentage` — AI contribution to code output
- `ai_efficiency_index` — Composite AI efficiency score (volume, usage, cost, churn)

**Quality:**
- `bugs_introduced` — Bugs introduced
- `bug_ratio` — Bug-to-feature ratio
- `revert_prs` — Reverted pull requests
- `code_turnover` — Code churn rate

## Best practices

1. **Always specify date ranges.** Use `start_time` and `end_time` in `yyyy-mm-dd` format.
2. **Look up IDs first.** Before filtering by team or person, call `get_accounts` or `get_teams` to get the correct IDs.
3. **Start with overview, then drill down.** Use `get_metric_overview` for trends, then `get_metric_drill_down` to investigate specifics.
4. **Use group_by for comparisons.** In `get_metric_overview`, use `group_by` to compare across teams, people, or repositories.
5. **Present data clearly.** Round numbers, use tables, and compare against benchmarks when available.
